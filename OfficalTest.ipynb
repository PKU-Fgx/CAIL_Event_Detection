{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "575907f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import jsonlines\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "from model import BertCRFForTokenClassification\n",
    "from torch.utils.data import DataLoader, SequentialSampler, TensorDataset\n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66bfa992",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"\n",
    "pretrained_path = \"/tf/FangGexiang/3.SememeV2/pretrained_model/bert-base-chinese\"\n",
    "pretrained_model_path = \"/tf/FangGexiang/1.CAILED/ModelSaved/OfficalBaselineV1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a14d4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputExample(object):\n",
    "    \"\"\"\n",
    "        A single training/test example for token classification.\n",
    "        one single sequence of tokens is an example in LEVEN task.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, guid, words, labels):\n",
    "        self.guid = guid\n",
    "        self.words = words\n",
    "        self.labels = labels\n",
    "\n",
    "\n",
    "class InputFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self, input_ids, input_mask, segment_ids, label_ids):\n",
    "        self.input_ids = input_ids\n",
    "        self.input_mask = input_mask\n",
    "        self.segment_ids = segment_ids\n",
    "        self.label_ids = label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35836798",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_examples_from_file(data_dir):\n",
    "    input_data = jsonlines.open(data_dir)\n",
    "    examples = []\n",
    "\n",
    "    for doc in tqdm(input_data, desc=\"[1] Reading Examples from file...\"):\n",
    "        words = [c['tokens'] for c in doc['content']]\n",
    "        labels = [['O']*len(c['tokens']) for c in doc['content']]\n",
    "\n",
    "        for i in range(0, len(words)):\n",
    "            examples.append(InputExample(guid=\"%s-%d\" % (doc['id'], i),\n",
    "                                         words=words[i],\n",
    "                                         labels=labels[i]))\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9206afe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_examples_to_features(\n",
    "    examples,\n",
    "    label_list,\n",
    "    max_seq_length,\n",
    "    tokenizer,\n",
    "    cls_token_at_end=False,\n",
    "    cls_token=\"[CLS]\",\n",
    "    cls_token_segment_id=0,\n",
    "    sep_token=\"[SEP]\",\n",
    "    sep_token_extra=False,\n",
    "    pad_on_left=False,\n",
    "    pad_token=0,\n",
    "    pad_token_segment_id=0,\n",
    "    pad_token_label_id=-100,\n",
    "    sequence_a_segment_id=0,\n",
    "    mask_padding_with_zero=True,\n",
    "    model_name=None\n",
    "):\n",
    "    \"\"\" Loads a data file into a list of `InputBatch`s\n",
    "        `cls_token_at_end` define the location of the CLS token:\n",
    "            - False (Default, BERT/XLM pattern): [CLS] + A + [SEP] + B + [SEP]\n",
    "            - True (XLNet/GPT pattern): A + [SEP] + B + [SEP] + [CLS]\n",
    "        `cls_token_segment_id` define the segment id associated to the CLS token (0 for BERT, 2 for XLNet)\n",
    "    \"\"\"\n",
    "\n",
    "    label_map = {label: i for i, label in enumerate(label_list)}\n",
    "\n",
    "    # my logic in crf_padding requires this check. I create mask for crf by labels==pad_token_label_id to not include it\n",
    "    # in loss and decoding\n",
    "    assert pad_token_label_id not in label_map.values()\n",
    "\n",
    "    features = []\n",
    "    for ex_index, example in enumerate(tqdm(examples, desc=\"[2] Converting Examples to Features\")):\n",
    "\n",
    "        tokens = []\n",
    "        label_ids = []\n",
    "        for word, label in zip(example.words, example.labels):\n",
    "            word_tokens = tokenizer.tokenize(word)\n",
    "            if len(word_tokens) == 0:\n",
    "                word_tokens = ['<UNK>']\n",
    "            tokens.extend(word_tokens)\n",
    "            # Use the real label id for the first token of the word, and padding ids for the remaining tokens\n",
    "            label_ids.extend([label_map[label]] + [pad_token_label_id] * (len(word_tokens) - 1))\n",
    "\n",
    "        # Account for [CLS] and [SEP] with \"- 2\" and with \"- 3\" for RoBERTa.\n",
    "        special_tokens_count = 3 if sep_token_extra else 2\n",
    "        if len(tokens) > max_seq_length - special_tokens_count:\n",
    "            tokens = tokens[:(max_seq_length - special_tokens_count)]\n",
    "            label_ids = label_ids[:(max_seq_length - special_tokens_count)]\n",
    "\n",
    "        '''\n",
    "        The convention in BERT is:\n",
    "        (a) For sequence pairs:\n",
    "         tokens:   [CLS] is this jack ##son ##ville ? [SEP] no it is not . [SEP]\n",
    "         type_ids:   0   0  0    0    0     0       0   0   1  1  1  1   1   1\n",
    "        (b) For single sequences:\n",
    "         tokens:   [CLS] the dog is hairy . [SEP]\n",
    "         type_ids:   0   0   0   0  0     0   0\n",
    "        Where \"type_ids\" are used to indicate whether this is the first\n",
    "        sequence or the second sequence. The embedding vectors for `type=0` and\n",
    "        `type=1` were learned during pre-training and are added to the wordpiece\n",
    "        embedding vector (and position vector). This is not *strictly* necessary\n",
    "        since the [SEP] token unambiguously separates the sequences, but it makes\n",
    "        it easier for the model to learn the concept of sequences.\n",
    "        '''\n",
    "\n",
    "        tokens += [sep_token]\n",
    "        label_ids += [pad_token_label_id]       # [label_map[\"X\"]]\n",
    "\n",
    "        if sep_token_extra:\n",
    "            # roberta uses an extra separator b/w pairs of sentences\n",
    "            tokens += [sep_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "        segment_ids = [sequence_a_segment_id] * len(tokens)\n",
    "\n",
    "        if cls_token_at_end:\n",
    "            tokens += [cls_token]\n",
    "            label_ids += [pad_token_label_id]\n",
    "            segment_ids += [cls_token_segment_id]\n",
    "        else:\n",
    "            tokens = [cls_token] + tokens\n",
    "            label_ids = [pad_token_label_id] + label_ids\n",
    "            segment_ids = [cls_token_segment_id] + segment_ids\n",
    "\n",
    "        if model_name:\n",
    "            if model_name == 'xlm-roberta-base':\n",
    "                tokenizer = XLMRobertaTokenizer.from_pretrained(model_name)\n",
    "                input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            elif model_name.startswith('bert'):\n",
    "                tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "                input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "            elif model_name == 'roberta':\n",
    "                tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "                input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "        else:\n",
    "            input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        padding_length = max_seq_length - len(input_ids)\n",
    "        if pad_on_left:\n",
    "            input_ids = ([pad_token] * padding_length) + input_ids\n",
    "            input_mask = ([0 if mask_padding_with_zero else 1] * padding_length) + input_mask\n",
    "            segment_ids = ([pad_token_segment_id] * padding_length) + segment_ids\n",
    "            label_ids = ([pad_token_label_id] * padding_length) + label_ids\n",
    "\n",
    "        else:\n",
    "            input_ids += ([pad_token] * padding_length)\n",
    "            input_mask += ([0 if mask_padding_with_zero else 1] * padding_length)\n",
    "            segment_ids += ([pad_token_segment_id] * padding_length)\n",
    "            label_ids += ([pad_token_label_id] * padding_length)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "        assert len(label_ids) == max_seq_length\n",
    "\n",
    "        features.append(\n",
    "            InputFeatures(input_ids=input_ids,\n",
    "                          input_mask=input_mask,\n",
    "                          segment_ids=segment_ids,\n",
    "                          label_ids=label_ids))\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a23b43db",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_cache_examples(tokenizer, labels, pad_token_label_id):\n",
    "    # Load data features from cache or dataset file\n",
    "    cached_features_file = \"/tf/FangGexiang/1.CAILED/Data/Cache/cached_test_stage_1\"\n",
    "    if os.path.exists(cached_features_file):\n",
    "        features = torch.load(cached_features_file)\n",
    "    else:\n",
    "        examples = read_examples_from_file(\"/tf/FangGexiang/1.CAILED/Data/test_stage1.jsonl\")\n",
    "        features = convert_examples_to_features(\n",
    "            examples, \n",
    "            labels, \n",
    "            512,\n",
    "            tokenizer,\n",
    "            cls_token_at_end=False,           # xlnet has a cls token at the end\n",
    "            cls_token=tokenizer.cls_token,\n",
    "            cls_token_segment_id=0,\n",
    "            sep_token=tokenizer.sep_token,\n",
    "            sep_token_extra=False,            # roberta uses an extra separator b/w pairs of sentences\n",
    "            pad_on_left=False,                # pad on the left for xlnet\n",
    "            pad_token=tokenizer.convert_tokens_to_ids([tokenizer.pad_token])[0],\n",
    "            pad_token_segment_id=0,\n",
    "            pad_token_label_id=pad_token_label_id\n",
    "        )\n",
    "        torch.save(features, cached_features_file)\n",
    "\n",
    "    # Convert to Tensors and build dataset\n",
    "    all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n",
    "    all_input_mask = torch.tensor([f.input_mask for f in features], dtype=torch.long)\n",
    "    all_segment_ids = torch.tensor([f.segment_ids for f in features], dtype=torch.long)\n",
    "    all_label_ids = torch.tensor([f.label_ids for f in features], dtype=torch.long)\n",
    "\n",
    "    dataset = TensorDataset(all_input_ids, all_input_mask, all_segment_ids, all_label_ids)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a94b3d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, tokenizer, labels, pad_token_label_id):\n",
    "    print(\"正在读取数据...\")\n",
    "    eval_dataset = load_and_cache_examples(tokenizer, labels, pad_token_label_id)\n",
    "\n",
    "    eval_batch_size = 32\n",
    "    # Note that DistributedSampler samples randomly\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(eval_dataset, sampler=eval_sampler, batch_size=eval_batch_size)\n",
    "\n",
    "    # Eval!\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    preds = None\n",
    "    out_label_ids = None\n",
    "    model.eval()\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\"):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = {\n",
    "                \"input_ids\": batch[0],\n",
    "                \"attention_mask\": batch[1],\n",
    "                \"token_type_ids\": batch[2]\n",
    "            }\n",
    "            best_path = model(pad_token_label_id=pad_token_label_id, **inputs)\n",
    "\n",
    "        inputs.update({\"labels\": batch[3]})\n",
    "        nb_eval_steps += 1\n",
    "        if preds is None:\n",
    "            preds = best_path.detach().cpu().numpy()\n",
    "            out_label_ids = inputs[\"labels\"].detach().cpu().numpy()\n",
    "        else:\n",
    "            preds = np.append(preds, best_path.detach().cpu().numpy(), axis=0)\n",
    "            out_label_ids = np.append(out_label_ids, inputs[\"labels\"].detach().cpu().numpy(), axis=0)\n",
    "\n",
    "    label_map = {i: label for i, label in enumerate(labels)}\n",
    "\n",
    "    preds_list = [[] for _ in range(out_label_ids.shape[0])]\n",
    "\n",
    "    for i in range(out_label_ids.shape[0]):\n",
    "        for j in range(out_label_ids.shape[1]):\n",
    "            if out_label_ids[i, j] != pad_token_label_id:\n",
    "                preds_list[i].append(label_map[preds[i][j]])\n",
    "\n",
    "    return preds_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "68e0af92",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = eval(open(\"/tf/FangGexiang/1.CAILED/Data/bio_label.txt\", \"r\").readline())\n",
    "num_labels = len(labels)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_path, use_fast=True)\n",
    "model_config = AutoConfig.from_pretrained(pretrained_model_path, num_labels=num_labels)\n",
    "\n",
    "ner_model = BertCRFForTokenClassification.from_pretrained(pretrained_model_path, config=model_config)\n",
    "ner_model = ner_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a2f6c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在读取数据...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:  47%|████▋     | 1132/2421 [22:39<26:30,  1.23s/it]"
     ]
    }
   ],
   "source": [
    "predictions = evaluate(ner_model, tokenizer, labels, -100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1b5de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test_predictions_file = \"./results.jsonl\"\n",
    "pure_event2id = eval(open(\"/tf/FangGexiang/1.CAILED/Data/label.txt\", \"r\").readline())\n",
    "with open(output_test_predictions_file, \"w\") as writer:\n",
    "    Cnt = 0\n",
    "    levenTypes = list(pure_event2id.keys())\n",
    "    with open(\"/tf/FangGexiang/1.CAILED/Data/test_stage1.jsonl\", \"r\") as fin:\n",
    "        lines = fin.readlines()\n",
    "        for line in tqdm(lines, desc=\"Writing...\"):\n",
    "            doc = json.loads(line)\n",
    "            res = {}\n",
    "            res['id'] = doc['id']\n",
    "            res['predictions'] = []\n",
    "            for mention in doc['candidates']:\n",
    "                if mention['offset'][1] > len(predictions[Cnt + mention['sent_id']]):\n",
    "                    print(len(doc['content'][mention['sent_id']]['tokens']),\n",
    "                          len(predictions[Cnt + mention['sent_id']]))\n",
    "                    res['predictions'].append({\"id\": mention['id'], \"type_id\": 0})\n",
    "                    continue\n",
    "                is_NA = False if predictions[Cnt + mention['sent_id']][mention['offset'][0]].startswith(\n",
    "                    \"B\") else True\n",
    "                if not is_NA:\n",
    "                    Type = predictions[Cnt + mention['sent_id']][mention['offset'][0]][2:]\n",
    "                    for i in range(mention['offset'][0] + 1, mention['offset'][1]):\n",
    "                        if predictions[Cnt + mention['sent_id']][i][2:] != Type:\n",
    "                            is_NA = True\n",
    "                            break\n",
    "                    if not is_NA:\n",
    "                        res['predictions'].append({\"id\": mention['id'], \"type_id\": levenTypes.index(Type)})\n",
    "                if is_NA:\n",
    "                    res['predictions'].append({\"id\": mention['id'], \"type_id\": 0})\n",
    "            writer.write(json.dumps(res) + \"\\n\")\n",
    "            Cnt += len(doc['content'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
